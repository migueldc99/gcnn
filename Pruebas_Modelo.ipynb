{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef31d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice notebook environment\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# Importing numpy\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn import GraphConv, SumPooling, AvgPooling\n",
    "from dgl.nn.pytorch import Sequential\n",
    "\n",
    "# Importing matplotlib for graphics and fixing the default size of plots\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors # para colores en las grÃ¡ficas\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset']='cm'\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "# In case we need sympy\n",
    "from sympy import init_printing\n",
    "init_printing(use_latex=True)\n",
    "\n",
    "# For working with well stabished parameters\n",
    "from mendeleev import element\n",
    "from scipy.constants import physical_constants\n",
    "\n",
    "import tarfile   # para tratar los archivos .tar\n",
    "import glob      # para buscar directorios\n",
    "import re        # para expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee381433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDens(nn.Module):\n",
    "    \n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "    \n",
    "    LinearDens is introduced as a way of use nn.Linear together with GraphConv since Sequential() \n",
    "    requires the layers to be joined have the same input. In this case, a dgl.graph and a features tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "    \n",
    "        super(LinearDens, self).__init__()\n",
    "    \n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "    \n",
    "    def forward(self, graph, feat, weight=None, edge_weight=None):\n",
    "        \n",
    "        return self.linear(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d70948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_potential(n_gc_layers: int = 1,\n",
    "                    n_fc_layers: int = 1,\n",
    "                    activation_function: str = 'Tanh',\n",
    "                    pooling: str = 'add',\n",
    "                    n_node_features: int = 55,\n",
    "                    n_edge_features: int = 1,\n",
    "                    n_neurons: int = None ) -> torch.nn.Module: \n",
    "\n",
    "    r\"\"\" This is a function that returns a model with the desired number of Graph-Convolutional (n_gc_layers) layers and \n",
    "    Fully-Connected (n_fc_layers) linear layers, using the specified non-linear activation layers interspaced\n",
    "    between them. \n",
    "\n",
    "    Args:\n",
    "        \n",
    "        n_gc_layers (int): number of Graph-Convolutional layers (GraphConv from dgl). Default: 1 \n",
    "        n_fc_layers (int): number of densely-connected -Fully Connected- linear layers (LinearDens). Default: 1\n",
    "        activation_function (char): nonlinear activation layer; can be any activation available in torch. Default: Tanh\n",
    "        pooling (char): the type of pooling of node results. If set to 'add' performs a sum over all nodes (e.g. return total energy).\n",
    "                        In other case, performs the 'mean' to return energy per atom. Default: Add\n",
    "        n_node_features (int): length of node feature vectors. Default: 55\n",
    "        \n",
    "        \n",
    "        n_edge_features (int): length of edge feature vectors; currently 1, the distance.\n",
    "        ATENCION PQ CON LA FUNCION QUE ESTAMOS USANDO NO ES NECESARIO SABER LOS EDGES\n",
    "        \n",
    "        \n",
    "        n_neurons (int): number of neurons in deep layers (if they exist) in the densely-connected network.  \n",
    "                       If set to None, it is internally set to n_node_features. Default: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # First, we define the activation layer, the pooling layer and the number of neurons:\n",
    "    \n",
    "    activation_layer = eval(\"torch.nn.\" + activation_function + \"()\")\n",
    "    \n",
    "    if pooling == 'add':\n",
    "        poolingLayer = SumPooling()\n",
    "   \n",
    "    else:\n",
    "        poolingLayer = AvgPooling()\n",
    "\n",
    "    if n_neurons == None:\n",
    "        n_neurons = n_node_features\n",
    "\n",
    "    \n",
    "    # Now, the Graph-convolutional layers: \n",
    "\n",
    "    conv = GraphConv(in_feats = n_node_features, out_feats = n_node_features, activation = activation_layer)\n",
    "    \n",
    "    model = conv\n",
    "      \n",
    "    for n in range(1, n_gc_layers-1):\n",
    "        \n",
    "        model = Sequential(model, conv)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Then the fully-connected layers: \n",
    "\n",
    "    linear1 = LinearDens(n_node_features, 1)           # only 1 fully connected layer; last conv layer --> final result)\n",
    "    linear2 = LinearDens(n_node_features, n_neurons)   # conv layer --> fully connected layer\n",
    "    linear3 = LinearDens(n_neurons, n_neurons)         # fully connected layer --> fully connected layer\n",
    "    linear4 = LinearDens(n_neurons, 1)                 # fully connected layer --> final result\n",
    "    \n",
    "    \n",
    "    if n_fc_layers == 1:\n",
    "        \n",
    "        model = Sequential(model, linear1)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model = Sequential(model, linear2)\n",
    "        \n",
    "        model = Sequential(model, activation_layer)\n",
    "        \n",
    "        for n in range(1, n_fc_layers-1):\n",
    "            \n",
    "            model = Sequential(model, linear3)\n",
    "            \n",
    "            model= Sequential(model, activation_layer)\n",
    "            \n",
    "        # and finally the exit layer\n",
    "\n",
    "        model = Sequential(model, linear4)\n",
    "    \n",
    "    # last but not least, the pooling layer\n",
    "\n",
    "    model = Sequential(model, poolingLayer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295bc653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_node_features = 3\n",
      "Nodes g:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.0841, 0.1991, 0.3221],\n",
      "        [0.2027, 0.4560, 0.6721],\n",
      "        [0.2027, 0.4560, 0.6721],\n",
      "        [0.2027, 0.4560, 0.6721],\n",
      "        [0.2027, 0.4560, 0.6721],\n",
      "        [0.2027, 0.4560, 0.6721]], grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.0154,  0.0007,  0.0947],\n",
      "        [-0.0639,  0.0248,  0.4009],\n",
      "        [-0.0639,  0.0248,  0.4009],\n",
      "        [-0.0639,  0.0248,  0.4009],\n",
      "        [-0.0639,  0.0248,  0.4009],\n",
      "        [-0.0639,  0.0248,  0.4009]], grad_fn=<TanhBackward0>)\n",
      "tensor([[0.0779],\n",
      "        [0.1495],\n",
      "        [0.1495],\n",
      "        [0.1495],\n",
      "        [0.1495],\n",
      "        [0.1495]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0779],\n",
      "        [0.1495],\n",
      "        [0.1495],\n",
      "        [0.1495],\n",
      "        [0.1495],\n",
      "        [0.1495]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2287]], grad_fn=<SegmentReduceBackward>)\n"
     ]
    }
   ],
   "source": [
    "####PARTE DE PRUEBAS#####\n",
    "\n",
    "\n",
    "#Creamos un grafo sencillo\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]), num_nodes=6)\n",
    "g = dgl.add_self_loop(g)\n",
    "#print(g.num_nodes())\n",
    "\n",
    "g.ndata['fit'] = torch.ones(g.num_nodes(), 3)\n",
    "print('n_node_features =', g.ndata['fit'].shape[1])\n",
    "g.edata['a'] = torch.randn(g.num_edges(), 4)\n",
    "print('Nodes g:\\n',g.ndata['fit'])\n",
    "\n",
    "activation_function = 'Tanh'\n",
    "activation_layer = eval(\"torch.nn.\" + activation_function + \"()\")\n",
    "\n",
    "conv = GraphConv(in_feats = 3, out_feats = 3, activation= activation_layer)  # allow_zero_in_degree = True\n",
    "conv2 = GraphConv(in_feats = 3, out_feats = 2, activation= activation_layer)\n",
    "lin = LinearDens(2, 1)\n",
    "\n",
    "res = conv(g, g.ndata['fit'])\n",
    "#print(res)\n",
    "res = conv(g, res)\n",
    "#print(res)\n",
    "res = conv2(g, res)\n",
    "#print(res)\n",
    "res = lin(g, res)\n",
    "#print(res)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential(conv, conv)\n",
    "model = Sequential(model, conv2)\n",
    "model = Sequential(model, lin)\n",
    "res = model(g, g.ndata['fit'])\n",
    "#print(res)\n",
    "\n",
    "\n",
    "model2 = graph_potential(n_gc_layers = 3, n_fc_layers = 1, n_node_features = g.ndata['fit'].shape[1], n_edge_features=3)\n",
    "res = model2(g, g.ndata['fit'])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6136545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
