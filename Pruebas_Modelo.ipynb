{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef31d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice notebook environment\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# Importing numpy\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "# Importing matplotlib for graphics and fixing the default size of plots\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors # para colores en las gráficas\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset']='cm'\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "\n",
    "# In case we need sympy\n",
    "from sympy import init_printing\n",
    "init_printing(use_latex=True)\n",
    "\n",
    "# For working with well stabished parameters\n",
    "from mendeleev import element\n",
    "from scipy.constants import physical_constants\n",
    "\n",
    "import tarfile   # para tratar los archivos .tar\n",
    "import glob      # para buscar directorios\n",
    "import re        # para expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33d70948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn import GraphConv, DenseGraphConv, SumPooling, AvgPooling\n",
    "from dgl.nn.pytorch import Sequential\n",
    "\n",
    "\n",
    "def graph_potential(n_gc_layers: int = 1,\n",
    "                    n_fc_layers: int = 1,\n",
    "                    activation_function: str = 'Tanh',\n",
    "                    pooling: str = 'add',\n",
    "                    n_node_features: int = 55,\n",
    "                    n_edge_features: int = 1,\n",
    "                    n_neurons: int = None) -> torch.nn.Module: \n",
    "\n",
    "    r\"\"\" \n",
    "    A function that returns a model with the desired number of Graph-Convolutional (n_gc_layers) layers and \n",
    "    Fully-Connected (n_fc_layers) linear layers, using the specified non-linear activation layers interspaced\n",
    "    between them. \n",
    "\n",
    "    Args:\n",
    "       \n",
    "        n_gc_layers (int): number of Graph-Convolution layers (GraphConv from dgl); default: 1 \n",
    "        n_fc_layers (int): number of densely-connected -Fully Connected- linear layers (DenseGraphConv); default: 1\n",
    "        activation_function (char): nonlinear activation layer; can be any activation available in torch; default: Tanh\n",
    "        pooling (char): the type of pooling of node results; can be 'add', i.e. summing over all nodes (e.g. to \n",
    "                return total energy; default) or 'mean', to return energy per atom.\n",
    "        n_node_features (int): length of node feature vectors; default: 55\n",
    "        \n",
    "        \n",
    "        n_edge_features (int): length of edge feature vectors; currently 1, the distance.\n",
    "        ATENCION PQ CON LA FUNCION QUE ESTAMOS USANDO NO ES NECESARIO SABER LOS EDGES\n",
    "        \n",
    "        n_neurons (int): number of neurons in deep layers (if they exist) in the densely-connected network.  \n",
    "                       If set to None, it is internally set to n_node_features; default: None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    activation_layer = eval(\"torch.nn.\" + activation_function + \"()\")\n",
    "    \n",
    "    if pooling == 'add':    \n",
    "        \n",
    "        poolingLayer = SumPooling()\n",
    "   \n",
    "    else:\n",
    "    \n",
    "        poolingLayer = AvgPooling()\n",
    "\n",
    "    if n_neurons == None:\n",
    "        \n",
    "        n_neurons = n_node_features\n",
    "\n",
    "    \n",
    "    # first the Graph-convolutional layers: \n",
    "    \n",
    "    r\"\"\"     \n",
    "        GraphConv(in_feats, out_feats, norm='both', weight=True, bias=True, activation=None, allow_zero_in_degree=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    conv = GraphConv(in_feats = n_node_features, out_feats = n_node_features, activation = activation_layer)\n",
    "    # Hacemos la convolución y activación en un sólo comando, luego a layers append se mete directo\n",
    "    \n",
    "    model = conv\n",
    "    print(model(g, g.ndata['fit']))\n",
    "    \n",
    "    for n in range(0, n_gc_layers):\n",
    "    \n",
    "        model = Sequential(model, conv)\n",
    "        print(model(g, g.ndata['fit']))\n",
    "        \n",
    "    # then the fully-connected layers: \n",
    "    \n",
    "    # Applies a linear transformation to the incoming data: y = xA^T + b\n",
    "    linear1 = nn.Linear(n_node_features, 1)           # only 1 fully connected layer \n",
    "                                                      # (has n_node_features as it is connected to the last conv layer)\n",
    "    linear2 = nn.Linear(n_node_features, n_neurons)   # conv layer --> fully connected layer\n",
    "    linear3 = nn.Linear(n_neurons, n_neurons)         # fully connected layer --> fully connected layer\n",
    "    linear4 = nn.Linear(n_neurons, 1)                 # fully connected layer --> final result\n",
    "    \n",
    "    \n",
    "    if n_fc_layers == 1:\n",
    "        \n",
    "        model = Sequential(model, linear1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        model = Sequential(model, linear2)\n",
    "        print(\"Hola\")\n",
    "        print(model(g, g.ndata['fit']))\n",
    "        model = Sequential(model, activation_layer)  ### aqui hay que hacer una activation layer sobre la que hemos hecho la transformacion lineal\n",
    "        \n",
    "        for n in range(1, n_fc_layers-1):\n",
    "            \n",
    "            model = Sequential(model, linear3)\n",
    "            \n",
    "            model= Sequential(model, activation_layer)\n",
    "            \n",
    "        # and finally the exit layer\n",
    "        \n",
    "        model = Sequential(model, linear4)\n",
    "    \n",
    "    # last but not least, the pooling layer\n",
    "\n",
    "    model = Sequential(model, poolingLayer) # Quitarlo de aquí, en todo caso entre conv layers\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "295bc653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_node_features = 3\n",
      "Nodes g:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 0.4950, -0.1238,  0.2440],\n",
      "        [ 0.8677, -0.2945,  0.5422],\n",
      "        [ 0.8677, -0.2945,  0.5422],\n",
      "        [ 0.8677, -0.2945,  0.5422],\n",
      "        [ 0.8677, -0.2945,  0.5422],\n",
      "        [ 0.8677, -0.2945,  0.5422]], grad_fn=<TanhBackward0>)\n",
      "tensor([[0.2420, 0.0119, 0.0166],\n",
      "        [0.7562, 0.0449, 0.0685],\n",
      "        [0.7562, 0.0449, 0.0685],\n",
      "        [0.7562, 0.0449, 0.0685],\n",
      "        [0.7562, 0.0449, 0.0685],\n",
      "        [0.7562, 0.0449, 0.0685]], grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.0102,  0.0409],\n",
      "        [-0.0686,  0.2354],\n",
      "        [-0.0686,  0.2354],\n",
      "        [-0.0686,  0.2354],\n",
      "        [-0.0686,  0.2354],\n",
      "        [-0.0686,  0.2354]], grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.6770],\n",
      "        [-0.5694],\n",
      "        [-0.5694],\n",
      "        [-0.5694],\n",
      "        [-0.5694],\n",
      "        [-0.5694]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3326,  0.3159,  0.0410],\n",
      "        [-0.6876,  0.6627,  0.0998],\n",
      "        [-0.6876,  0.6627,  0.0998],\n",
      "        [-0.6876,  0.6627,  0.0998],\n",
      "        [-0.6876,  0.6627,  0.0998],\n",
      "        [-0.6876,  0.6627,  0.0998]], grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.0693, -0.0772, -0.1254],\n",
      "        [-0.2920, -0.3171, -0.4949],\n",
      "        [-0.2920, -0.3171, -0.4949],\n",
      "        [-0.2920, -0.3171, -0.4949],\n",
      "        [-0.2920, -0.3171, -0.4949],\n",
      "        [-0.2920, -0.3171, -0.4949]], grad_fn=<TanhBackward0>)\n",
      "tensor([[ 0.0205, -0.0307, -0.0036],\n",
      "        [ 0.1638, -0.2333, -0.0293],\n",
      "        [ 0.1638, -0.2333, -0.0293],\n",
      "        [ 0.1638, -0.2333, -0.0293],\n",
      "        [ 0.1638, -0.2333, -0.0293],\n",
      "        [ 0.1638, -0.2333, -0.0293]], grad_fn=<TanhBackward0>)\n",
      "tensor([[0.0081, 0.0050, 0.0099],\n",
      "        [0.1097, 0.0720, 0.1395],\n",
      "        [0.1097, 0.0720, 0.1395],\n",
      "        [0.1097, 0.0720, 0.1395],\n",
      "        [0.1097, 0.0720, 0.1395],\n",
      "        [0.1097, 0.0720, 0.1395]], grad_fn=<TanhBackward0>)\n",
      "Hola\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12228/3080803316.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#print(res)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_potential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_gc_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_fc_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_node_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_edge_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#n_edge_features = g.edata['a'].shape[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12228/2493826261.py\u001b[0m in \u001b[0;36mgraph_potential\u001b[1;34m(n_gc_layers, n_fc_layers, activation_function, pooling, n_node_features, n_edge_features, n_neurons)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hola\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_layer\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m### aqui hay que hacer una activation layer sobre la que hemos hecho la transformacion lineal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\dgl\\nn\\pytorch\\utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, graph, *feats)\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                     \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m                 \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             raise TypeError('The first argument of forward must be a DGLGraph'\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "#Creamos un grafo sencillo\n",
    "g = dgl.graph(([0, 0, 0, 0, 0], [1, 2, 3, 4, 5]), num_nodes=6)\n",
    "g = dgl.add_self_loop(g)\n",
    "#print(g.num_nodes())\n",
    "#print(g.num_edges())\n",
    "\n",
    "g.ndata['fit'] = torch.ones(g.num_nodes(), 3)\n",
    "print('n_node_features =', g.ndata['fit'].shape[1])\n",
    "g.edata['a'] = torch.randn(g.num_edges(), 4)\n",
    "print('Nodes g:\\n',g.ndata['fit'])\n",
    "\n",
    "activation_function = 'Tanh'\n",
    "activation_layer = eval(\"torch.nn.\" + activation_function + \"()\")\n",
    "\n",
    "conv = GraphConv(in_feats = 3, out_feats = 3, activation= activation_layer)  # allow_zero_in_degree = True\n",
    "conv2 = GraphConv(in_feats = 3, out_feats = 2, activation= activation_layer)\n",
    "lin1 = nn.Linear(2,1)\n",
    "res = conv(g, g.ndata['fit'])\n",
    "print(res)\n",
    "res = conv(g, res)\n",
    "print(res)\n",
    "res = conv2(g, res)\n",
    "print(res)\n",
    "res = lin1(res)\n",
    "print(res)\n",
    "\n",
    "#model = Sequential(conv, conv)\n",
    "#model = Sequential(model, conv2)\n",
    "#res = model(g, g.ndata['fit'])\n",
    "#print(res)\n",
    "\n",
    "model2 = graph_potential(n_gc_layers = 3, n_fc_layers = 3, n_node_features = g.ndata['fit'].shape[1], n_edge_features=3)\n",
    "\n",
    "#n_edge_features = g.edata['a'].shape[1]\n",
    "res = model2(g, g.ndata['fit'])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813e2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6136545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
